{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IykHejam2Vo"
   },
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaU9U_U_m2Vp"
   },
   "source": [
    "# Density estimation using Real NVP\n",
    "\n",
    "Normalizing flows are the class of probabilistic models that provides flexible parametrical probabilistic models, where the probability density function can be computed exactly. In the assignment, we will consider a real-valued non-volume preserving normalizing flows (Real NVP) -- a special case of normalizing flow.\n",
    "\n",
    "#### Problem setting\n",
    "\n",
    "Our goal is to training a generative network $g_\\theta:  Z \\rightarrow X, g = f^{-1}$ that maps latent variable $z \\sim p(z)$ to a sample $x \\sim p(x)$. Where $p(z)$ is a prior distibiution and $p(x)$ is a data distibution. An illustrative example is provided below. \n",
    "\n",
    "<img src=\"2d-example.png\" width=600px>\n",
    "\n",
    "\n",
    "#### Change of variable formula\n",
    "\n",
    "Given an observed data variable $x \\in X$,\n",
    "a simple prior probability distribution $p_{Z}$ on a latent variable $z \\in Z$,\n",
    "and a bijection $f: X \\rightarrow Z$ (with $g = f^{-1}$),\n",
    "the change of variable formula defines a model distribution on $X$ by\n",
    "\\begin{align}\n",
    "p_{X}(x) &= p_{Z}\\big(f(x)\\big) \\left|\\det\\left(\\cfrac{\\partial f(x)}{\\partial x^T} \\right)\\right|\n",
    "\\label{eq:change-variables}\\\\\n",
    "\\log\\left(p_{X}(x)\\right) &= \\log\\Big(p_{Z}\\big(f(x)\\big)\\Big) + \\log\\left(\\left|\\det\\left(\\frac{\\partial f(x)}{\\partial x^T}\\right)\\right|\\right)\n",
    ",\n",
    "\\end{align}\n",
    "where $\\frac{\\partial f(x)}{\\partial x^T}$ is the Jacobian of $f$ at $x$.\n",
    "\n",
    "Exact samples from the resulting distribution can be generated by using the inverse transform sampling rule. A sample $z \\sim p_{Z}$ is drawn in the latent space, and its inverse image $x = f^{-1}(z) = g(z)$ generates a sample in the original space. Computing the density on a point $x$ is accomplished by computing the density of its image $f(x)$ and multiplying by the associated Jacobian determinant $\\det\\left(\\frac{\\partial f(x)}{\\partial x^T}\\right)$.\n",
    "\n",
    "#### Real NVP\n",
    "\n",
    "Real NVP presents a class of functions where $\\log\\left(\\left|\\det\\left(\\frac{\\partial f(x)}{\\partial x^T}\\right)\\right|\\right)$ can be computed efficiently (see, 3.3 Properties, https://arxiv.org/abs/1605.08803). Every layer of Real NVP is a coupling layer followed by permutation layer. Combination of coupling and permutation layers can be implemented as a masked version of the coupling layer:\n",
    "\\begin{align}\n",
    "y = b \\odot x + (1 - b) \\odot \\Big(x \\odot \\exp\\big(s(b \\odot x)\\big) + t(b \\odot x)\\Big),\n",
    "\\end{align}\n",
    "\n",
    "where $s$ and $t$ stand for scale and translation, and are functions from $R^{D} \\mapsto R^{D}$, and $\\odot$ is the Hadamard product or element-wise product, $b$ is a binary mask. For more details on the model see the paper Density estimation using Real NVP https://arxiv.org/abs/1605.08803.\n",
    "\n",
    "# In this assignment:\n",
    "1. Implementation of Real NVP\n",
    "2. Training Real NVP on 2d circles or moons dataset\n",
    "3. Visualization of the generative model\n",
    "4. Optional Research Assignment\n",
    "\n",
    "Additional information:\n",
    "- You will need the following python packages: pytorch, numpy, sklearn, pylab (matplotlib).\n",
    "- If you have an urgent question or find a typo or a mistake, send it to ars.ashuha@gmail.com. The title should include \"BDL Assignment 2 2018\".\n",
    "- A submission policy will be released later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw9U5gBJm2Vr"
   },
   "source": [
    "# Implementation of Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ck7vMI5fm2Vr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nSKa5num2Vu"
   },
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets, nett, mask, prior):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        # Create a flow\n",
    "        # nets:  a function that return a pytocrn neurel network e.g., nn.Sequential, s = nets(), s: dim(X) -> dim(X)\n",
    "        # nett:  a function that return a pytocrn neurel network e.g., nn.Sequential, t = nett(), t: dim(X) -> dim(X)\n",
    "        # mask:  a torch.Tensor of size #number_of_coupling_layers x #dim(X)\n",
    "        # prior: an object from torch.distributions e.g., torch.distributions.MultivariateNormal\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(masks))])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(masks))])\n",
    "        \n",
    "    def g(self, z):\n",
    "        # Compute and return g(z) = x, \n",
    "        #    where self.mask[i], self.t[i], self.s[i] define a i-th masked coupling layer   \n",
    "        # z: a torch.Tensor of shape batchSize x 1 x dim(X)\n",
    "        # return x: a torch.Tensor of shape batchSize x 1 x dim(X)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        # Compute f(x) = z and log_det_Jakobian of f, \n",
    "        #    where self.mask[i], self.t[i], self.s[i] define a i-th masked coupling layer   \n",
    "        # x: a torch.Tensor, of shape batchSize x dim(X), is a datapoint\n",
    "        # return z: a torch.Tensor of shape batchSize x dim(X), a hidden representations\n",
    "        # return log_det_J: a torch.Tensor of len batchSize\n",
    "        \n",
    "        return z, log_det_J\n",
    "    \n",
    "    def log_prob(self,x):\n",
    "        # Compute and return log p(x)\n",
    "        # using the change of variable formula and log_det_J computed by f\n",
    "        # return logp: torch.Tensor of len batchSize\n",
    "        return logp\n",
    "        \n",
    "    def sample(self, batchSize): \n",
    "        # Draw and return batchSize samples from flow using implementation of g\n",
    "        # return x: torch.Tensor of shape batchSize x 1 x dim(X)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVEmgjY6m2Vx"
   },
   "outputs": [],
   "source": [
    "nets = # a function that take no arguments and return a pytorch model, dim(X) -> dim(X)\n",
    "nett = # a function that take no arguments and return a pytorch model, dim(X) -> dim(X)\n",
    "\n",
    "# Check nets and nett are working i.e., computing without errors\n",
    "# Check that resulting dimensions s and t are the same and equal dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h41avAbYm2V0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "masks = # torch.Tensor of size #number_of_coupling_layers x #dim(X)\n",
    "# Check that when dim(X) == 2, the mask for every layer has just one 1.0 and one 0.0 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8j9Gl2ekm2V2"
   },
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "# Check that prior has log_prob and sample methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGhCIeg-m2V5"
   },
   "outputs": [],
   "source": [
    "flow = RealNVP(nets, nett, masks, prior)\n",
    "# Check that a flow is reversible g(f(x)) = x\n",
    "# With a big chance you have some errors in RealNVP.log_prob, think hard on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65cHS8_hm2V6"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "trainable_parametrs = # list of all trainable parameters in a flow\n",
    "optimizer = # choose an optimizer, use module torch.optim\n",
    "\n",
    "for t in range(5001):    \n",
    "    noisy_circles = datasets.make_circles(n_samples=100, factor=.5, noise=.05)\n",
    "    loss = # compute the maximum-likelihood loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)\n",
    "        \n",
    "# Check that the loss decreases\n",
    "# Is the visualization below good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD_1WB_gm2V9"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfP7SH65m2V9"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "noisy_circles = datasets.make_circles(n_samples=100, factor=.5, noise=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2C-HOP6hm2WB"
   },
   "source": [
    "Draw several plots: \n",
    "- samples from flow\n",
    "- samples from prior\n",
    "- data samples\n",
    "- mapping form data to prior\n",
    "\n",
    "The goal is to obtain figure similar to https://arxiv.org/abs/1605.08803"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KIVPO-Am2WC"
   },
   "source": [
    "## Providde answerrs for the folowing questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCiTW8w_m2WD"
   },
   "source": [
    "1. What architecture worked better in your experiments?\n",
    "\n",
    "\n",
    "**Your answer with justification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mH_2XoyVm2WE"
   },
   "source": [
    "2. Did you find what is important for stable training (initializations, nonlinearities, ...)?\n",
    "\n",
    "\n",
    "**Your answer with justification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBCfQewqm2WF"
   },
   "source": [
    "3. How convergence speed (in iterations) depends on the complexity of architecture?\n",
    "\n",
    "\n",
    "**Your answer with justification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ny0tcR10m2WH"
   },
   "source": [
    "# Optional Research Assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjS_fZxNm2WH"
   },
   "source": [
    "This assignment is optional. It will give you up to 2 additional points for one of the assignment.   \n",
    "#### 1. VAE: \n",
    "Use normalizing flow to get more expressive $q(z\\,|\\,x)$,  https://arxiv.org/abs/1505.05770. TL;dr; use decoder to predict $z_0 \\sim q_0(z_0|x)$, than use NF to obtain $z_k = f_k \\circ \\dots \\circ f_1(z)$ sample from a more flexible distribution. Compare it with a conventional normal distribution, what is working better? Compare it with conventional VAE  that uses the same number of parameters as VAE with NF.\n",
    "\n",
    "#### 2. Expressiveness: \n",
    "- Train the flow on an another 2d-dataset e.g., a mixture of 6 Gaussians (see figure 10, https://openreview.net/pdf?id=Hkg313AcFX). Is it possible to fit it with a normalizing flow? What is more beneficial increase size of s and t or increase the depth? Provide a justification for the answer e.g. plots.\n",
    "- Use the flow to sample images from a more complex dataset, e.g. downsampled MNIST (8x8 should be fine). Do your findings remains the same? Also, provide plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHt_MDA4m2WI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bdl-assignment2-nf.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
