{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SparseVD-assignment.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"8KxW17ljMVbF","colab_type":"text"},"cell_type":"markdown","source":["# Assignment 3"]},{"metadata":{"id":"tsBLwpKdMVbH","colab_type":"text"},"cell_type":"markdown","source":["# Variational Dropout Sparsifies Deep Neural Networks\n","\n","The Variational Dropout (https://arxiv.org/abs/1506.02557) provides a Bayesian interpretation of conventional dropout procedure. Later it was shown that Variational Dropout can be used for model sparsification (Sparse VD), the effect can be achieved by via optimization of variational lower bound wrt individual dropout rates for every weight of the model (https://arxiv.org/abs/1701.05369).\n","\n","#### Sparse VD\n","\n","Sparse VD model optimizes VLB $\\mathcal{L}(\\phi)$ with respect to parameters $\\phi$ of a variational approximation $q_\\phi(w)$:\n","\n","$$\\mathcal{L}(\\phi) =  L_\\mathcal{D}(\\phi) - D_{KL}(q_\\phi(w)\\,\\|\\,p(w)) \\to\\max_{\\phi\\in\\Phi}$$\n","$$L_\\mathcal{D}(\\phi) = \\sum_{n=1}^N \\mathrm{E}_{q_\\phi(w)}[\\log p(y_n\\,|\\,x_n, w)],$$\n","\n","where $p(w)$ is the log-uniform prior distibution, the variational approxinmation $q_\\phi(w)$ is a fullly factorized gaussian, the likelihood $p(y\\,|\\,x, w)$ is defined by a neuralnework with parametrs $w$. The optimization is performed by stohasic optimization methods e.g., Adam, etc.\n","\n","For more convenience computing, the KL divergence is approximated as follows:\n","\\begin{equation}\n","\\begin{gathered}\n","    -D_{KL}(q(w_{ij}\\,|\\,\\theta_{ij}, \\alpha_{ij})\\,\\|\\,p(w_{ij})) \\approx\n","    \\\\\n","    \\approx k_1\\sigma(k_2 + k_3\\log \\alpha_{ij})) - 0.5\\log(1+\\alpha_{ij}^{-1}) + \\mathrm{C}\n","    \\label{eq:KL}\\\\\n","    k_1=0.63576~~~~~k_2=1.87320~~~~~k_3=1.48695\n","\\end{gathered}\n","\\end{equation}\n","\n","**Note:** In the paper two parametrizations of q are used. The fist one is $\\phi_i=\\{\\mu_{i}, \\sigma_i\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\sigma^2_{ij})$ and the second one is $\\phi_{ij}=\\{\\mu_{ij}, \\alpha_{ij}\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\alpha_{ij}\\mu^2_{ij})$. This two parametrization are connected as $\\sigma^2_{ij} = \\alpha_{ij}\\mu^2_{ij}$. Do not be confused.\n","\n","\n","# In this assignment:\n","1. Implementation of fully-connected Sparse VD layer\n","2. Training Lenet-300-100 on MNIST dataset\n","3. Optional Research Assignment\n","\n","Additional information:\n","- If you have a problem with importing logger, download logger.py and file to the same folder and run a notebook from it\n","- You will need the following python packages: pytorch, numpy, sklearn, pylab (matplotlib), tabulate\n","- If you have an urgent question or find a typo or a mistake, send it to ars.ashuha@gmail.com. The title should include \"BDL Assignment 3, 2018\""]},{"metadata":{"id":"KsimFopvMVbI","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","import torch\n","import numpy as np\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from logger import Logger\n","from torch.nn import Parameter\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dztg-5hDMVbM","colab_type":"text"},"cell_type":"markdown","source":["## Implementation of  Sparse VD layer"]},{"metadata":{"id":"rskYuZWkMVbN","colab_type":"code","colab":{}},"cell_type":"code","source":["class LinearSVDO(nn.Module):\n","    def __init__(self, in_features, out_features, threshold, bias=True):\n","        super(LinearSVDO, self).__init__()\n","        \"\"\"\n","            in_features: int, a number of input features\n","            out_features: int, a number of neurons\n","            threshold: float, a threshold for clipping weights\n","        \"\"\"\n","        \n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.threshold = threshold\n","\n","        self.mu = # torch.nn.parameter.Parameter of size out_features x in_features\n","        self.log_sigma = # torch.nn.parameter.Parameter of size out_features x in_features\n","        self.bias = # torch.nn.parameter.Parameter of size 1 x out_features\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        self.bias.data.zero_()\n","        self.mu.data.normal_(0, 0.02)\n","        self.log_sigma.data.fill_(-5)        \n","        \n","    def forward(self, x):      \n","        # x is a torch.Tensor of shape (?number_of_objects, in_features)\n","        # log_alpha is a torch.Tensor of shape (out_features, in_features)\n","        self.log_alpha = # Compute using self.log_sigma and self.mu\n","        # clipping for a numerical stability\n","        self.log_alpha = torch.clamp(self.log_alpha, -10, 10)   \n","        \n","        if self.training:\n","            # lrt_mean is a torch.Tensor of shape (x.shape[0], out_features)\n","            lrt_mean =  # compute mean activation using LRT\n","            # lrt_std is a torch.Tensor of shape (x.shape[0], out_features)\n","            lrt_std = # compute std of activations unsig lrt, \n","                      # do not forget use torch.sqrt(x + 1e-8) instead of torch.sqrt(x)\n","            # eps is a torch.Tensor of shape (x.shape[0], out_features)\n","            eps = # sample of noise for reparametrization\n","            return # sample of activation\n","        \n","        out = # compute the output of the layer\n","        # use weighs W = Eq = self.mu\n","        # clip all weight with log_alpha > threshold\n","        return out\n","        \n","    def kl_reg(self):\n","        k1, k2, k3 = torch.Tensor([0.63576]), torch.Tensor([1.8732]), torch.Tensor([1.48695])\n","        # kl is a scalar torch.Tensor \n","        kl = # eval KL using the approximation\n","        return kl"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-uo1Ur-MMVbQ","colab_type":"text"},"cell_type":"markdown","source":["## Define LeNet-300-100"]},{"metadata":{"id":"9bZmsqU3MVbQ","colab_type":"code","colab":{}},"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, threshold):\n","        super(Net, self).__init__()\n","        self.fc1 = LinearSVDO(28*28, 300, threshold)\n","        self.fc2 = LinearSVDO(300,  100, threshold)\n","        self.fc3 = LinearSVDO(100,  10, threshold)\n","        self.threshold=threshold\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.log_softmax(self.fc3(x), dim=1)\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H-0I5jarMVbT","colab_type":"text"},"cell_type":"markdown","source":["## Function for loading MNIST"]},{"metadata":{"id":"2SjgqcgLMVbT","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_mnist(batch_size):\n","    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=True, download=True,\n","        transform=trsnform), batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=False, download=True,\n","        transform=trsnform), batch_size=batch_size, shuffle=True)\n","\n","    return train_loader, test_loader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1Jb8iFigMVbX","colab_type":"text"},"cell_type":"markdown","source":["## Create SGVLB loss"]},{"metadata":{"id":"6-oQVbqXMVbY","colab_type":"code","colab":{}},"cell_type":"code","source":["class SGVLB(nn.Module):\n","    def __init__(self, net, train_size):\n","        super(SGVLB, self).__init__()\n","        self.train_size = train_size # int, the len of dataset\n","        self.net = net # nn.Module\n","        \n","    def forward(self, input, target, kl_weight=1.0):\n","        assert not target.requires_grad\n","        kl = 0.0\n","        for module in self.net.children():\n","            if hasattr(module, 'kl_reg'):\n","                kl = kl + module.kl_reg()\n","                \n","        sgvlb_loss = # a scalar torch.Tensor, SGVLB loss\n","        return sgvlb_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1xwIkQdlMVba","colab_type":"text"},"cell_type":"markdown","source":["## Define the model"]},{"metadata":{"id":"S7t1Me8eMVbb","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Net(threshold=3)\n","optimizer = # optimizer\n","scheduler = # decrease learning rate by torch.optim.lr_scheduler\n","\n","fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n","logger = Logger('sparse_vd', fmt=fmt)\n","\n","train_loader, test_loader = get_mnist(batch_size=100)\n","sgvlb = SGVLB(model, len(train_loader.dataset))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"06mxgwp2MVbe","colab_type":"text"},"cell_type":"markdown","source":["## Train the model"]},{"metadata":{"scrolled":false,"id":"MWinHfniMVbf","colab_type":"code","colab":{}},"cell_type":"code","source":["kl_weight = 0.02\n","epochs = 100\n","\n","for epoch in range(1, epochs + 1):\n","    scheduler.step()\n","    model.train()\n","    train_loss, train_acc = 0, 0 \n","    kl_weight = min(kl_weight+0.02, 1)\n","    logger.add_scalar(epoch, 'kl', kl_weight)\n","    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data = data.view(-1, 28*28)\n","        optimizer.zero_grad()\n","        \n","        output = model(data)\n","        pred = output.data.max(1)[1] \n","        loss = sgvlb(output, target, kl_weight)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += float(loss) \n","        train_acc += np.sum(pred.numpy() == target.data.numpy())\n","\n","    logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n","    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n","    \n","    model.eval()\n","    test_loss, test_acc = 0, 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        data = data.view(-1, 28*28)\n","        output = model(data)\n","        test_loss += float(sgvlb(output, target, kl_weight))\n","        pred = output.data.max(1)[1] \n","        test_acc += np.sum(pred.numpy() == target.data.numpy())\n","        \n","    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n","    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n","    \n","    for i, c in enumerate(model.children()):\n","        if hasattr(c, 'kl_reg'):\n","            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > model.threshold).mean())\n","            \n","    logger.iter_info()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s9BYVKxAMVbj","colab_type":"code","colab":{}},"cell_type":"code","source":["all_w, kep_w = 0, 0\n","\n","for c in model.children():\n","    kep_w += (c.log_alpha.data.numpy() < model.threshold).sum()\n","    all_w += c.log_alpha.data.numpy().size\n","\n","# compression_ratio shold be > 30\n","compression_ratio = all_w/kep_w\n","print('compression_ratio =', compression_ratio)\n","assert compression_ratio > 30"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WGpOF9mzMVbl","colab_type":"text"},"cell_type":"markdown","source":["## Disk space"]},{"metadata":{"id":"BjUuLQTQMVbl","colab_type":"code","colab":{}},"cell_type":"code","source":["import scipy\n","import numpy as np\n","from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n","\n","row, col, data = [], [], []\n","M = list(model.children())[0].W.data.numpy()\n","LA = list(model.children())[0].log_alpha.data.numpy()\n","\n","for i in range(300):\n","    for j in range(28*28):\n","        if LA[i, j] < 3:\n","            row += [i]\n","            col += [j]\n","            data += [M[i, j]]\n","\n","Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n","Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n","Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZzWIiMr3MVbn","colab_type":"code","colab":{}},"cell_type":"code","source":["np.savez_compressed('M_w', M)\n","scipy.sparse.save_npz('Mcsr_w', Mcsr)\n","scipy.sparse.save_npz('Mcsc_w', Mcsc)\n","scipy.sparse.save_npz('Mcoo_w', Mcoo)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HojA6M4_MVbp","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls -lah | grep .npz "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IXtYpKL7MVbs","colab_type":"text"},"cell_type":"markdown","source":["## Visualization"]},{"metadata":{"id":"MahGlBp_MVbt","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from matplotlib import rcParams\n","rcParams['figure.figsize'] = 16, 3\n","rcParams['figure.dpi'] = 300\n","\n","\n","log_alpha = (model.fc1.log_alpha.detach().numpy() < 3).astype(np.float)\n","W = model.fc1.W.detach().numpy()\n","\n","plt.imshow(log_alpha * W, cmap='hot', interpolation=None)\n","plt.colorbar()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RdKgBhs-MVbv","colab_type":"code","colab":{}},"cell_type":"code","source":["s = 0\n","from matplotlib import rcParams\n","rcParams['figure.figsize'] = 8, 5\n","\n","z = np.zeros((28*15, 28*15))\n","\n","for i in range(15):\n","    for j in range(15):\n","        s += 1\n","        z[i*28:(i+1)*28, j*28:(j+1)*28] =  np.abs((log_alpha * W)[s].reshape(28, 28))\n","        \n","plt.imshow(z, cmap='hot_r')\n","plt.colorbar()\n","plt.axis('off')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aHJdsdqAMVby","colab_type":"text"},"cell_type":"markdown","source":["# Optional Research Assignment (up to 2 points)"]},{"metadata":{"id":"cN8tSaevMVbz","colab_type":"text"},"cell_type":"markdown","source":["1. Study the model: \n","    - How sparsity and accuracy depend on maximum of KL-multiplier (kl_weight)?\n","    - How quality depends on the initialization of log_sigma (log_sigma)?\n","    - Study the KL approximation: what if we use the reparametrization trick to obtain an unbiased MC estimate of KL?\n","2. Compression:\n","    - What can we do to obtain better compression results with small quality degradation?\n","    - Propose and eval several options.\n","3. Study the Local reparametrization trick: \n","    - Does it really accelerate convergence?\n","    - Does variance of gradient decrease?\n","    \n","You can do one out of three parts. You need to provide evidence for results e.g., plots, etc."]},{"metadata":{"id":"4jNY6wg6MVb0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}